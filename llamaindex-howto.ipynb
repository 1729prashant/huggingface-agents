{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b7071-9c73-4129-9cd8-0e4c090af654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index-llms-huggingface\n",
    "#!pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface\n",
    "#!pip install llama-index\n",
    "# !pip install hf_xet\n",
    "#!pip install llama-index-vector-stores-chroma\n",
    "#!pip install -U llama-index-callbacks-arize-phoenix\n",
    "#!pip install llama-index-tools-google\n",
    "# !pip install llama-index-tools-mcp --issues with this\n",
    "!pip install llama-index-utils-workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57879423-190d-4cd8-8fd2-b921dcc4d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage of the Hugging Face inference API for an LLM component.\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve HF_TOKEN from the environment variables\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    "    token=hf_token,\n",
    "    provider=\"auto\"\n",
    ")\n",
    "\n",
    "response = llm.complete(\"Hello, how are you?\")\n",
    "print(response)\n",
    "# Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fa193-3ab6-4688-a3fa-da8b4e1cad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While LlamaIndex has many components, we’ll focus specifically on the QueryEngine component. Why? Because it \n",
    "# can be used as a Retrieval-Augmented Generation (RAG) tool for an agent.\n",
    "\n",
    "# So, what is RAG? LLMs are trained on enormous bodies of data to learn general knowledge. However, they may not \n",
    "# be trained on relevant and up-to-date data. RAG solves this problem by finding and retrieving relevant information \n",
    "# from your data and giving that to the LLM.\n",
    "\n",
    "\n",
    "# There are five key stages within RAG, which in turn will be a part of most larger applications you build. These are:\n",
    "\n",
    "# 1. Loading: this refers to getting your data from where it lives — whether it’s text files, PDFs, another website, \n",
    "#    a database, or an API — into your workflow. LlamaHub provides hundreds of integrations to choose from.\n",
    "\n",
    "# 2. Indexing: this means creating a data structure that allows for querying the data. For LLMs, this nearly always \n",
    "#    means creating vector embeddings. Which are numerical representations of the meaning of the data. Indexing can \n",
    "#    also refer to numerous other metadata strategies to make it easy to accurately find contextually relevant data based on properties.\n",
    "\n",
    "# 3. Storing: once your data is indexed you will want to store your index, as well as other metadata, to avoid having to re-index it.\n",
    "# 4. Querying: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures \n",
    "#    to query, including sub-queries, multi-step queries and hybrid strategies.\n",
    "\n",
    "# 5. Evaluation: a critical step in any flow is checking how effective it is relative to other strategies, or when you \n",
    "#    make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5eb3c6-bf55-446b-ac5f-c3f4ce555cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and embedding documents\n",
    "# As mentioned before, LlamaIndex can work on top of your own data, however, before accessing data, we need to load it. \n",
    "# There are three main ways to load data into LlamaIndex:\n",
    "# 1. SimpleDirectoryReader: A built-in loader for various file types from a local directory.\n",
    "# 2. LlamaParse: LlamaParse, LlamaIndex’s official tool for PDF parsing, available as a managed API.\n",
    "# 3. LlamaHub: A registry of hundreds of data-loading libraries to ingest data from any source.\n",
    "\n",
    "# SimpleDirectoryReader component can load various file types from a folder and convert them into Document objects that LlamaIndex\n",
    "#   from llama_index.core import SimpleDirectoryReader\n",
    "#   reader = SimpleDirectoryReader(input_dir=\"path/to/directory\")\n",
    "#   documents = reader.load_data()\n",
    "\n",
    "\n",
    "# After loading our documents, we need to break them into smaller pieces called Node objects. A Node is just a chunk \n",
    "# of text from the original document that’s easier for the AI to work with, while it still has references to the original \n",
    "# Document object.\n",
    "\n",
    "# The IngestionPipeline helps us create these nodes through two key transformations.\n",
    "\n",
    "# SentenceSplitter breaks down documents into manageable chunks by splitting them at natural sentence boundaries.\n",
    "# HuggingFaceEmbedding converts each chunk into numerical embeddings - vector representations that capture the semantic \n",
    "# meaning in a way AI can process efficiently.\n",
    "# This process helps us organise our documents in a way that’s more useful for searching and analysis.\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_overlap=0),\n",
    "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "nodes = await pipeline.arun(documents=[Document.example()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989776e2-ccae-480e-b6f3-4c11a7b7d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing and indexing documents\n",
    "# After creating our Node objects we need to index them to make them searchable, but before we can do that, we need a place \n",
    "# to store our data.\n",
    "# Since we are using an ingestion pipeline, we can directly attach a vector store to the pipeline to populate it. In this case,\n",
    "# we will use Chroma to store our documents.\n",
    "\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n",
    "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "\n",
    "# This is where vector embeddings come in - by embedding both the query and nodes in the same vector space, we can find \n",
    "# relevant matches. The VectorStoreIndex handles this for us, using the same embedding model we used during ingestion to \n",
    "# ensure consistency. Let’s see how to create this index from our vector store and embeddings:\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)\n",
    "\n",
    "# All information is automatically persisted within the ChromaVectorStore object and the passed directory path.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b81862-6185-4030-97eb-f0e1ceac14dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying a VectorStoreIndex with prompts and LLMs\n",
    "# Before we can query our index, we need to convert it to a query interface. The most common conversion options are:\n",
    "\n",
    "# -- as_retriever: For basic document retrieval, returning a list of NodeWithScore objects with similarity scores\n",
    "# -- as_query_engine: For single question-answer interactions, returning a written response\n",
    "# -- as_chat_engine: For conversational interactions that maintain memory across multiple messages, returning a written \n",
    "#                    response using chat history and indexed context\n",
    "# We’ll focus on the query engine since it is more common for agent-like interactions. We also pass in an LLM to the query \n",
    "# engine to use for the response.\n",
    "\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "query_engine.query(\"What is the meaning of life?\")\n",
    "# The meaning of life is 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d417d55-2277-4ca7-a33c-aa8e6b2e5850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Processing\n",
    "# Under the hood, the query engine doesn’t only use the LLM to answer the question but also uses a ResponseSynthesizer as a \n",
    "# strategy to process the response. Once again, this is fully customisable but there are three main strategies that work well \n",
    "# out of the box:\n",
    "\n",
    "# refine: create and refine an answer by sequentially going through each retrieved text chunk. This makes a separate LLM call \n",
    "#         per Node/retrieved chunk.\n",
    "# compact (default): similar to refining but concatenating the chunks beforehand, resulting in fewer LLM calls.\n",
    "# tree_summarize: create a detailed answer by going through each retrieved text chunk and creating a tree structure of the answer.\n",
    "\n",
    "# The language model won’t always perform in predictable ways, so we can’t be sure that the answer we get is always correct. \n",
    "# We can deal with this by evaluating the quality of the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672dc2c6-9f3f-4005-a400-6ec662baed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation and observability\n",
    "# LlamaIndex provides built-in evaluation tools to assess response quality. These evaluators leverage LLMs to analyze responses \n",
    "# across different dimensions. Let’s look at the three main evaluators available:\n",
    "# FaithfulnessEvaluator: Evaluates the faithfulness of the answer by checking if the answer is supported by the context.\n",
    "# AnswerRelevancyEvaluator: Evaluate the relevance of the answer by checking if the answer is relevant to the question.\n",
    "# CorrectnessEvaluator: Evaluate the correctness of the answer by checking if the answer is correct.\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "query_engine = # from the previous section\n",
    "llm = # from the previous section\n",
    "\n",
    "# query index\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "response = query_engine.query(\n",
    "    \"What battles took place in New York City in the American Revolution?\"\n",
    ")\n",
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "eval_result.passing\n",
    "\n",
    "\n",
    "# Even without direct evaluation, we can gain insights into how our system is performing through observability. This is especially \n",
    "# useful when we are building more complex workflows and want to understand how each component is performing.\n",
    "# we can install the LlamaTrace callback from Arize Phoenix with the following command: pip install -U llama-index-callbacks-arize-phoenix\n",
    "# Additionally, we need to set the PHOENIX_API_KEY environment variable to our LlamaTrace API key. We can get this by:\n",
    "# Creating an account at LlamaTrace - https://llamatrace.com/login\n",
    "# Generating an API key in your account settings\n",
    "# Using the API key in the code below to enable tracing\n",
    "\n",
    "import llama_index\n",
    "import os\n",
    "\n",
    "PHOENIX_API_KEY = \"<PHOENIX_API_KEY>\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
    "llama_index.core.set_global_handler(\n",
    "    \"arize_phoenix\",\n",
    "    endpoint=\"https://llamatrace.com/v1/traces\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ecc19f-e86c-4caf-8578-fdb8b184bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABOVE - how to use components to create a QueryEngine.\n",
    "# BELOW - how we can use the QueryEngine as a tool for an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976cda44-7799-4c22-a6da-1aee05f957e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are four main types of tools in LlamaIndex:\n",
    "\n",
    "# 1. FunctionTool: Convert any Python function into a tool that an agent can use. It automatically figures out how the function works.\n",
    "# 2. QueryEngineTool: A tool that lets agents use query engines. Since agents are built on query engines, they can also use other agents as tools.\n",
    "# 3. Toolspecs: Sets of tools created by the community, which often include tools for specific services like Gmail.\n",
    "# 4. Utility Tools: Special tools that help handle large amounts of data from other tools.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating a FunctionTool\n",
    "# A FunctionTool provides a simple way to wrap any Python function and make it available to an agent. You can pass either a \n",
    "# synchronous or asynchronous function to the tool, along with optional name and description parameters. The name and description \n",
    "# are particularly important as they help the agent understand when and how to use the tool effectively. Let’s look at how to \n",
    "# create a FunctionTool below and then call it.\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is sunny\"\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name=\"my_weather_tool\",\n",
    "    description=\"Useful for getting the weather for a given location.\",\n",
    ")\n",
    "tool.call(\"New York\")\n",
    "# When using an agent or LLM with function calling, the tool selected (and the arguments written for that tool) rely strongly \n",
    "# on the tool name and description of the purpose and arguments of the tool.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating a QueryEngineTool\n",
    "# The QueryEngine we defined in the previous unit can be easily transformed into a tool using the QueryEngineTool class. \n",
    "# Let’s see how to create a QueryEngineTool from a QueryEngine in the example below.\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "tool = QueryEngineTool.from_defaults(query_engine, name=\"some useful name\", description=\"some useful description\")\n",
    "\n",
    "\n",
    "\n",
    "# Creating Toolspecs\n",
    "# Think of ToolSpecs as collections of tools that work together harmoniously - like a well-organized professional toolkit. \n",
    "# Just as a mechanic’s toolkit contains complementary tools that work together for vehicle repairs, a ToolSpec combines \n",
    "# related tools for specific purposes. For example, an accounting agent’s ToolSpec might elegantly integrate spreadsheet \n",
    "# capabilities, email functionality, and calculation tools to handle financial tasks with precision and efficiency.\n",
    "# And now we can load the toolspec and convert it to a list of tools.\n",
    "from llama_index.tools.google import GmailToolSpec\n",
    "\n",
    "tool_spec = GmailToolSpec()\n",
    "tool_spec_list = tool_spec.to_tool_list()\n",
    "# To get a more detailed view of the tools, we can take a look at the metadata of each tool.\n",
    "[(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]\n",
    "\n",
    "\n",
    "\n",
    "# LlamaIndex also allows using MCP tools through a ToolSpec on the LlamaHub. You can simply run an MCP server and start \n",
    "# using it through the following implementation.\n",
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "\n",
    "# We consider there is a mcp server running on 127.0.0.1:8000, or you can use the mcp client to connect to your own mcp server.\n",
    "mcp_client = BasicMCPClient(\"http://127.0.0.1:8000/sse\")\n",
    "mcp_tool = McpToolSpec(client=mcp_client)\n",
    "\n",
    "# get the agent\n",
    "agent = await get_agent(mcp_tool)\n",
    "\n",
    "# create the agent context\n",
    "agent_context = Context(agent)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Utility Tools\n",
    "# Oftentimes, directly querying an API can return an excessive amount of data, some of which may be irrelevant, overflow \n",
    "# the context window of the LLM, or unnecessarily increase the number of tokens that you are using. Let’s walk through our \n",
    "# two main utility tools below.\n",
    "\n",
    "# OnDemandToolLoader: This tool turns any existing LlamaIndex data loader (BaseReader class) into a tool that an agent can use. \n",
    "# The tool can be called with all the parameters needed to trigger load_data from the data loader, along with a natural language \n",
    "# query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then \n",
    "# query it ‘on-demand’. All three of these steps happen in a single tool call.\n",
    "\n",
    "# LoadAndSearchToolSpec: The LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements to_tool_list, \n",
    "# and when that function is called, two tools are returned: a loading tool and then a search tool. The load Tool execution would call \n",
    "# the underlying Tool, and then index the output (by default with a vector index). The search Tool execution would take in a query \n",
    "# string as input and call the underlying index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c0285-f8ad-46c5-be05-73e1d68a9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Agents in LlamaIndex\n",
    "\n",
    "# LlamaIndex supports three main types of reasoning agents:\n",
    "# 1. Function Calling Agents - These work with AI models that can call specific functions.\n",
    "# 2. ReAct Agents - These can work with any AI that does chat or text endpoint and deal with complex reasoning tasks.\n",
    "# 3. Advanced Custom Agents - These use more complex methods to deal with more complex tasks and workflows.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialising Agents\n",
    "# To create an agent, we start by providing it with a set of functions/tools that define its capabilities. \n",
    "# Let’s look at how to create an agent with some basic tools. As of this writing, the agent will automatically \n",
    "# use the function calling API (if available), or a standard ReAct agent loop.\n",
    "\n",
    "# LLMs that support a tools/functions API are relatively new, but they provide a powerful way to call tools by \n",
    "# avoiding specific prompting and allowing the LLM to create tool calls based on provided schemas.\n",
    "\n",
    "# ReAct agents are also good at complex reasoning tasks and can work with any LLM that has chat or text completion \n",
    "# capabilities. They are more verbose, and show the reasoning behind certain actions that they take.\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# define sample Tool -- type annotations, function names, and docstrings, are all included in parsed schemas!\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two integers and returns the resulting integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# initialize llm\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "# initialize agent\n",
    "agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [FunctionTool.from_defaults(multiply)],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Agents are stateless by default, however, they can remember past interactions using a Context object. This might \n",
    "# be useful if you want to use an agent that needs to remember previous interactions, like a chatbot that maintains \n",
    "# context across multiple messages or a task manager that needs to track progress over time.\n",
    "# stateless\n",
    "response = await agent.run(\"What is 2 times 2?\")\n",
    "\n",
    "# remembering state\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "ctx = Context(agent)\n",
    "\n",
    "response = await agent.run(\"My name is Bob.\", ctx=ctx)\n",
    "response = await agent.run(\"What was my name again?\", ctx=ctx)\n",
    "# You’ll notice that agents in LlamaIndex are async because they use Python’s await operator.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating RAG Agents with QueryEngineTools\n",
    "# Agentic RAG is a powerful way to use agents to answer questions about your data. We can pass various tools to Alfred \n",
    "# to help him answer questions. However, instead of answering the question on top of documents automatically, Alfred can \n",
    "# decide to use any other tool or flow to answer the question.\n",
    "# It is easy to wrap QueryEngine as a tool for an agent. When doing so, we need to define a name and description. \n",
    "# The LLM will use this information to correctly use the tool. Let’s see how to load in a QueryEngineTool using the \n",
    "# QueryEngine we created in the component section.\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm, similarity_top_k=3) # as shown in the Components in LlamaIndex section\n",
    "\n",
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"name\",\n",
    "    description=\"a specific description\",\n",
    "    return_direct=False,\n",
    ")\n",
    "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [query_engine_tool],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that has access to a database containing persona descriptions. \"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Creating Multi-agent systems\n",
    "# The AgentWorkflow class also directly supports multi-agent systems. By giving each agent a name and description, \n",
    "# the system maintains a single active speaker, with each agent having the ability to hand off to another agent.\n",
    "# By narrowing the scope of each agent, we can help increase their general accuracy when responding to user messages.\n",
    "# Agents in LlamaIndex can also directly be used as tools for other agents, for more complex and custom scenarios.\n",
    "from llama_index.core.agent.workflow import (\n",
    "    AgentWorkflow,\n",
    "    FunctionAgent,\n",
    "    ReActAgent,\n",
    ")\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two numbers.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "# Create agent configs\n",
    "# NOTE: we can use FunctionAgent or ReActAgent here.\n",
    "# FunctionAgent works for LLMs with a function calling API.\n",
    "# ReActAgent works for any LLM.\n",
    "calculator_agent = ReActAgent(\n",
    "    name=\"calculator\",\n",
    "    description=\"Performs basic arithmetic operations\",\n",
    "    system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\",\n",
    "    tools=[add, subtract],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "query_agent = ReActAgent(\n",
    "    name=\"info_lookup\",\n",
    "    description=\"Looks up information about XYZ\",\n",
    "    system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\",\n",
    "    tools=[query_engine_tool],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Create and run the workflow\n",
    "agent = AgentWorkflow(\n",
    "    agents=[calculator_agent, query_agent], root_agent=\"calculator\"\n",
    ")\n",
    "\n",
    "# Run the system\n",
    "response = await agent.run(user_msg=\"Can you add 5 and 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a427c9c-7323-4f93-a9d9-68345970aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating agentic workflows in LlamaIndex\n",
    "\n",
    "# Basic Workflow Creation\n",
    "from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
    "        # do something here\n",
    "        return StopEvent(result=\"Hello, world!\")\n",
    "\n",
    "w = MyWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Connecting Multiple Steps\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
    "        # Process initial data\n",
    "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result\n",
    "# The type hinting is important here, as it ensures that the workflow is executed correctly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loops and Branches\n",
    "# example of creating a loop by using the union operator |. In the example below, we see that the LoopEvent \n",
    "# is taken as input for the step and can also be returned as output.\n",
    "from llama_index.core.workflow import Event\n",
    "import random\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "class LoopEvent(Event):\n",
    "    loop_output: str\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print(\"Bad thing happened\")\n",
    "            return LoopEvent(loop_output=\"Back to step one.\")\n",
    "        else:\n",
    "            print(\"Good thing happened\")\n",
    "            return ProcessingEvent(intermediate_result=\"First step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "w = MultiStepWorkflow(verbose=False)\n",
    "result = await w.run()\n",
    "result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Drawing Workflows\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "w = ... # as defined in the previous section\n",
    "draw_all_possible_flows(w, \"flow.html\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# State Management\n",
    "# State management is useful when you want to keep track of the state of the workflow, so that every step \n",
    "# has access to the same state. We can do this by using the Context type hint on top of a parameter in \n",
    "# the step function.\n",
    "from llama_index.core.workflow import Context, StartEvent, StopEvent\n",
    "\n",
    "@step\n",
    "async def query(self, ctx: Context, ev: StartEvent) -> StopEvent:\n",
    "    # store query in the context\n",
    "    await ctx.store.set(\"query\", \"What is the capital of France?\")\n",
    "\n",
    "    # do something with context and event\n",
    "    val = ...\n",
    "\n",
    "    # retrieve query from the context\n",
    "    query = await ctx.store.get(\"query\")\n",
    "\n",
    "    return StopEvent(result=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65887499-770d-4c8b-8804-de1359994193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automating workflows with Multi-Agent Workflows\n",
    "\n",
    "# Instead of manual workflow creation, we can use the AgentWorkflow class to create a multi-agent workflow. \n",
    "# The AgentWorkflow uses Workflow Agents to allow you to create a system of one or more agents that can collaborate \n",
    "# and hand off tasks to each other based on their specialized capabilities. This enables building complex agent \n",
    "# systems where different agents handle different aspects of a task. Instead of importing classes from \n",
    "# llama_index.core.agent, we will import the agent classes from llama_index.core.agent.workflow. One agent must be \n",
    "# designated as the root agent in the AgentWorkflow constructor. \n",
    "# When a user message comes in, it is first routed to the root agent. Each agent can then:\n",
    "# - Handle the request directly using their tools\n",
    "# - Handoff to another agent better suited for the task\n",
    "# - Return a response to the user\n",
    "\n",
    "from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "# we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description\n",
    "multiply_agent = ReActAgent(\n",
    "    name=\"multiply_agent\",\n",
    "    description=\"Is able to multiply two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\",\n",
    "    tools=[multiply],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "addition_agent = ReActAgent(\n",
    "    name=\"add_agent\",\n",
    "    description=\"Is able to add two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to add numbers.\",\n",
    "    tools=[add],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Create the workflow\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[multiply_agent, addition_agent],\n",
    "    root_agent=\"multiply_agent\",\n",
    ")\n",
    "\n",
    "# Run the system\n",
    "response = await workflow.run(user_msg=\"Can you add 5 and 3?\")\n",
    "\n",
    "\n",
    "\n",
    "# Agent tools can also modify the workflow state we mentioned earlier. Before starting the workflow, we can provide \n",
    "# an initial state dict that will be available to all agents. The state is stored in the state key of the workflow \n",
    "# context. It will be injected into the state_prompt which augments each new user message.\n",
    "# Let’s inject a counter to count function calls by modifying the previous example:\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# Define some tools\n",
    "async def add(ctx: Context, a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    # update our count\n",
    "    cur_state = await ctx.store.get(\"state\")\n",
    "    cur_state[\"num_fn_calls\"] += 1\n",
    "    await ctx.store.set(\"state\", cur_state)\n",
    "\n",
    "    return a + b\n",
    "\n",
    "async def multiply(ctx: Context, a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    # update our count\n",
    "    cur_state = await ctx.store.get(\"state\")\n",
    "    cur_state[\"num_fn_calls\"] += 1\n",
    "    await ctx.store.set(\"state\", cur_state)\n",
    "\n",
    "    return a * b\n",
    "\n",
    "...\n",
    "\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[multiply_agent, addition_agent],\n",
    "    root_agent=\"multiply_agent\",\n",
    "    initial_state={\"num_fn_calls\": 0},\n",
    "    state_prompt=\"Current state: {state}. User message: {msg}\",\n",
    ")\n",
    "\n",
    "# run the workflow with context\n",
    "ctx = Context(workflow)\n",
    "response = await workflow.run(user_msg=\"Can you add 5 and 3?\", ctx=ctx)\n",
    "\n",
    "# pull out and inspect the state\n",
    "state = await ctx.store.get(\"state\")\n",
    "print(state[\"num_fn_calls\"])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
